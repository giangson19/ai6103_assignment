# AI6103 Deep Learning - Individual Assignment

## Effects of Hyper-parameters on Optimization and Regularization of Deep Neural Networks

A key challenge in deep learning is balancing optimization and regularization. Optimization is determined by the hyper-parameters initial learning rate and its schedule, while regularization is affected by weight decay and data augmentation. In this report, we explore how these hyper-parameters impact deep neural networks using the MobileNet architecture and the CIFAR-100 dataset. We found that setting a good learning rate plays an important role in model training, using a Cosine Annealing schedule and Weight Decay improves model performance. Additional experiments reveal that using ReLU activation function helps avoid the vanishing gradient problem and leads to smooth loss reduction.
