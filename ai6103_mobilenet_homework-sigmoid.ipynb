{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iG9Az0z2t7ye"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# Set seed for PyTorch (CPU & CUDA)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)  # For multi-GPU\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mobilenet import MobileNet\n",
    "from data import get_train_valid_loader, get_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hlgcY9H185AF"
   },
   "outputs": [],
   "source": [
    "def train_func(model, optimizer, criterion, train_loader, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def eval(model, criterion, test_loader, device):\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    valid_loss = valid_loss / len(valid_loader)\n",
    "    valid_accuracy = 100 * correct / total\n",
    "\n",
    "    return valid_loss, valid_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "data_dir = './data'\n",
    "train_loader, valid_loader = get_train_valid_loader(data_dir=data_dir,batch_size=batch_size,augment=True,random_seed=42)\n",
    "test_loader = get_test_loader(data_dir=data_dir, batch_size=batch_size)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "momentum = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: Training with weight decay 0.0005 and cosine annealing learning rate\n",
      "Epoch [1/300], Train Loss: 4.2304, Train Acc: 5.42%, Valid Loss: 3.8526, Valid Acc: 9.19%, Learning Rate: 0.049999\n",
      "Epoch [11/300], Train Loss: 2.0492, Train Acc: 43.84%, Valid Loss: 2.3076, Valid Acc: 39.09%, Learning Rate: 0.049834\n",
      "Epoch [21/300], Train Loss: 1.7521, Train Acc: 50.82%, Valid Loss: 3.3615, Valid Acc: 26.01%, Learning Rate: 0.049398\n",
      "Epoch [31/300], Train Loss: 1.6076, Train Acc: 54.28%, Valid Loss: 2.7026, Valid Acc: 32.12%, Learning Rate: 0.048694\n",
      "Epoch [41/300], Train Loss: 1.4984, Train Acc: 56.88%, Valid Loss: 2.2101, Valid Acc: 42.92%, Learning Rate: 0.047731\n",
      "Epoch [51/300], Train Loss: 1.4218, Train Acc: 58.83%, Valid Loss: 2.4895, Valid Acc: 38.29%, Learning Rate: 0.046519\n",
      "Epoch [61/300], Train Loss: 1.3477, Train Acc: 60.55%, Valid Loss: 3.2364, Valid Acc: 27.10%, Learning Rate: 0.045070\n",
      "Epoch [71/300], Train Loss: 1.2793, Train Acc: 62.50%, Valid Loss: 3.0323, Valid Acc: 31.95%, Learning Rate: 0.043402\n",
      "Epoch [81/300], Train Loss: 1.2265, Train Acc: 63.62%, Valid Loss: 2.3492, Valid Acc: 40.57%, Learning Rate: 0.041533\n",
      "Epoch [91/300], Train Loss: 1.1477, Train Acc: 66.05%, Valid Loss: 2.7156, Valid Acc: 37.87%, Learning Rate: 0.039482\n",
      "Epoch [101/300], Train Loss: 1.0900, Train Acc: 67.30%, Valid Loss: 2.8088, Valid Acc: 35.01%, Learning Rate: 0.037273\n",
      "Epoch [111/300], Train Loss: 1.0263, Train Acc: 69.06%, Valid Loss: 2.2557, Valid Acc: 44.04%, Learning Rate: 0.034929\n",
      "Epoch [121/300], Train Loss: 0.9674, Train Acc: 70.55%, Valid Loss: 2.2007, Valid Acc: 45.78%, Learning Rate: 0.032476\n",
      "Epoch [131/300], Train Loss: 0.8959, Train Acc: 72.44%, Valid Loss: 1.9401, Valid Acc: 50.64%, Learning Rate: 0.029941\n",
      "Epoch [141/300], Train Loss: 0.8190, Train Acc: 75.03%, Valid Loss: 2.4996, Valid Acc: 42.28%, Learning Rate: 0.027353\n",
      "Epoch [151/300], Train Loss: 0.7380, Train Acc: 77.44%, Valid Loss: 2.0399, Valid Acc: 49.90%, Learning Rate: 0.024738\n",
      "Epoch [161/300], Train Loss: 0.6666, Train Acc: 79.55%, Valid Loss: 2.1800, Valid Acc: 48.86%, Learning Rate: 0.022127\n",
      "Epoch [171/300], Train Loss: 0.5806, Train Acc: 81.97%, Valid Loss: 2.2022, Valid Acc: 48.67%, Learning Rate: 0.019546\n",
      "Epoch [181/300], Train Loss: 0.4926, Train Acc: 85.28%, Valid Loss: 2.2512, Valid Acc: 47.67%, Learning Rate: 0.017026\n",
      "Epoch [191/300], Train Loss: 0.4299, Train Acc: 87.42%, Valid Loss: 1.8649, Valid Acc: 55.51%, Learning Rate: 0.014593\n",
      "Epoch [201/300], Train Loss: 0.3539, Train Acc: 90.04%, Valid Loss: 2.1225, Valid Acc: 51.72%, Learning Rate: 0.012274\n",
      "Epoch [211/300], Train Loss: 0.2797, Train Acc: 92.54%, Valid Loss: 1.8323, Valid Acc: 56.07%, Learning Rate: 0.010094\n",
      "Epoch [221/300], Train Loss: 0.2261, Train Acc: 94.40%, Valid Loss: 1.7882, Valid Acc: 58.01%, Learning Rate: 0.008078\n",
      "Epoch [231/300], Train Loss: 0.1712, Train Acc: 96.34%, Valid Loss: 1.7002, Valid Acc: 59.85%, Learning Rate: 0.006247\n",
      "Epoch [241/300], Train Loss: 0.1313, Train Acc: 97.73%, Valid Loss: 1.6369, Valid Acc: 61.59%, Learning Rate: 0.004622\n",
      "Epoch [251/300], Train Loss: 0.0991, Train Acc: 98.59%, Valid Loss: 1.6013, Valid Acc: 62.11%, Learning Rate: 0.003220\n",
      "Epoch [261/300], Train Loss: 0.0767, Train Acc: 99.22%, Valid Loss: 1.5665, Valid Acc: 63.10%, Learning Rate: 0.002056\n",
      "Epoch [271/300], Train Loss: 0.0644, Train Acc: 99.47%, Valid Loss: 1.5447, Valid Acc: 63.42%, Learning Rate: 0.001144\n",
      "Epoch [281/300], Train Loss: 0.0578, Train Acc: 99.56%, Valid Loss: 1.5375, Valid Acc: 63.73%, Learning Rate: 0.000493\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for the new experiment\n",
    "num_epochs = 300\n",
    "learning_rate = 0.05\n",
    "weight_decays = [5e-4, 1e-4]\n",
    "\n",
    "# Function to train with weight decay and cosine annealing learning rate\n",
    "def train_weight_decay_cosine_annealing(model, initial_lr, weight_decay, criterion, train_loader, valid_loader, device, num_epochs):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = train_func(model, optimizer, criterion, train_loader, device, epoch)\n",
    "        valid_loss, valid_accuracy = eval(model, criterion, valid_loader, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Get the current learning rate\n",
    "        current_lr = scheduler.get_last_lr()[0]  # Extract current learning rate\n",
    "        if epoch % 10 == 0 or epoch == num_epochs-1:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, \"\n",
    "                  f\"Train Acc: {train_accuracy:.2f}%, \"\n",
    "                  f\"Valid Loss: {valid_loss:.4f}, \"\n",
    "                  f\"Valid Acc: {valid_accuracy:.2f}%, \"\n",
    "                  f\"Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "    return train_losses, valid_losses, train_accuracies, valid_accuracies\n",
    "    \n",
    "# Experiment: Train for 300 epochs with weight decay and cosine annealing learning rate\n",
    "# MobileNet model\n",
    "\n",
    "for weight_decay in weight_decays:\n",
    "    print(f\"Experiment: Training with weight decay {weight_decay} and cosine annealing learning rate\")\n",
    "    \n",
    "    model = MobileNet(num_classes=100, sigmoid_block_ind = [4,5,6,7,8,9,10]).to(device) # USING SIGMOID \n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "    train_losses, valid_losses, train_accuracies, valid_accuracies = train_weight_decay_cosine_annealing(\n",
    "        model, learning_rate, weight_decay, criterion, train_loader, valid_loader, device, num_epochs)\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(range(len(train_losses)), train_losses)\n",
    "    plt.plot(range(len(valid_losses)), valid_losses)\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"MobileNet with Weight Decay and Cosine Annealing LR: Loss vs Number of epochs\")\n",
    "    plt.legend(['train', 'valid'])\n",
    "    plt.savefig(f'images/sigmoid_weight_decay_cosine_annealing_loss_{weight_decay}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(range(len(train_accuracies)), train_accuracies)\n",
    "    plt.plot(range(len(valid_accuracies)), valid_accuracies)\n",
    "    plt.xlabel(\"Number of epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"MobileNet with Weight Decay and Cosine Annealing LR: Accuracy vs Number of epochs\")\n",
    "    plt.legend(['train', 'valid'])\n",
    "    plt.savefig(f'images/sigmoid_weight_decay_cosine_annealing_accuracy_{weight_decay}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    learning_curves = {\"train_losses\": train_losses,\n",
    "                                    \"valid_losses\": valid_losses,\n",
    "                                    \"train_accuracies\": train_accuracies,\n",
    "                                    \"valid_accuracies\": valid_accuracies\n",
    "                                        }\n",
    "    with open(f'learning_curves/sigmoid_cosine_annealing_weight_decay_{weight_decay}.json', 'w') as fp:\n",
    "        json.dump(learning_curves, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -m \"Auto-commit after activation function experiment\"\n",
    "!git push origin main  # Change \"main\" to your branch name"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
